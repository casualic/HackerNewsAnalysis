{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this project we will be analyzing the conent of Hackernews posts, and analyze their popularity. \n",
    "\n",
    "We will be using the following dataset:\n",
    "https://www.kaggle.com/hacker-news/hacker-news-posts\n",
    "\n",
    "It is a filtered dataset that takes a random sample of the submissions to Hackernews that have any activity on the submission (i.e they have a caomment), and then tandomly sampled. \n",
    "\n",
    "The headers are as follows:\n",
    "\n",
    "id: The unique identifier from Hacker News for the post\n",
    "title: The title of the post\n",
    "url: The URL that the posts links to, if it the post has a URL\n",
    "num_points: The number of points the post acquired, calculated as the total number of upvotes minus the total number of downvotes\n",
    "num_comments: The number of comments that were made on the post\n",
    "author: The username of the person who submitted the post\n",
    "created_at: The date and time at which the post was submitted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at'], ['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']]\n"
     ]
    }
   ],
   "source": [
    "from csv import reader\n",
    "\n",
    "opened_file = open('hacker_news.csv')\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "\n",
    "print(hn[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Will now clear up the headers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "\n",
      "\n",
      "[['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52'], ['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30'], ['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20'], ['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']]\n"
     ]
    }
   ],
   "source": [
    "headers = hn[0]\n",
    "hn = hn[1:]\n",
    "\n",
    "print(headers)\n",
    "\n",
    "print('\\n')\n",
    "#printing rows to verify\n",
    "\n",
    "print(hn[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will now be looking to filter by the category of posts which have ask HN or show HN , to investigate how well these do on average. These are 2 types of community posts, so we will investiagte if community posts tend to do better than other news articles and submissions. \n",
    "\n",
    "Luckily, when it is one of those types of posts, Hackernews starts the tile of the questions with Ask HN, and so we can simply extrapolate the entires which start with that particular string of 'Ask HN'.\n",
    "\n",
    "To do this we will create a loop to extrapolate a list with the requied titlkes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_posts length is 1744\n",
      "show_posts length is 1162\n",
      "other_posts length is 17194\n",
      "proportion of community posts is 0.1445771144278607\n"
     ]
    }
   ],
   "source": [
    "# initiate empty lists to operate on in the loop for the output\n",
    "\n",
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith('ask hn') == True:\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith('show hn') == True:\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "print('ask_posts length is ' + str(len(ask_posts)))        \n",
    "print('show_posts length is ' + str(len(show_posts)))        \n",
    "print('other_posts length is ' + str(len(other_posts)))\n",
    "\n",
    "print('proportion of community posts is ' + str((len(ask_posts) + len(show_posts)) / len(hn)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have isolated the types of posts we will see check their engagement on the comment basis:\n",
    "\n",
    "'num_comments' is column with index 4\n",
    "\n",
    "Firstly we will investigate the 'ask' posts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg ask comments: 14.038417431192661\n",
      "avg show comments: 10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "# set a ask comments list, to track total number of asks.\n",
    "# then find avg comments on an ask post\n",
    "\n",
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    num_comments = row[4]\n",
    "    num_comments = int(num_comments)\n",
    "    total_ask_comments += num_comments\n",
    "\n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "\n",
    "print('avg ask comments: ' + str(avg_ask_comments))\n",
    "\n",
    "# set a ask comments list, to track total number of asks.\n",
    "# then find avg comments on an ask post\n",
    "\n",
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    num_comments = row[4]\n",
    "    num_comments = int(num_comments)\n",
    "    total_show_comments += num_comments\n",
    "\n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "\n",
    "print('avg show comments: ' + str(avg_show_comments))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what we can see above, the average Ask comments receive around 14 comments on avareage, vs show comments of 10.3. \n",
    "\n",
    "This is somewhat expects, as the ask HN posts, directly ask for community engagement, and such prompting more comments to be created. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this we wil focus our analysis of the posts of these type, as they seem to acheive good community engagement. \n",
    "\n",
    "We will now try to see if the popularity of a post, is related to the time posted. \n",
    "\n",
    "To do this we will calculate:\n",
    "1. Amount of ask posts created in each hour of the dat, alongside the number of comments received\n",
    "2. Average number of comments the ask posts receive by the hour created\n",
    "\n",
    "We also note that the the time in the database is stored as a string, so we will want to convert these in to objects, so taht we can manipulate them more easily. \n",
    "\n",
    "For Reference, a sample element looks like the following:\n",
    "\n",
    "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['8/16/2016 9:55', '6'], ['11/22/2015 13:43', '29'], ['5/2/2016 10:14', '1'], ['8/2/2016 14:20', '3'], ['10/15/2015 16:38', '17']]\n",
      "\n",
      "\n",
      "These are the post counts by hour:\n",
      "\n",
      "{0: 55, 1: 60, 2: 58, 3: 54, 4: 47, 5: 46, 6: 44, 7: 34, 8: 48, 9: 45, 10: 59, 11: 58, 12: 73, 13: 85, 14: 107, 15: 116, 16: 108, 17: 100, 18: 109, 19: 110, 20: 80, 21: 109, 22: 71, 23: 68}\n",
      "\n",
      "\n",
      "These are the comment counts by hour: \n",
      "\n",
      "{0: 447, 1: 683, 2: 1381, 3: 421, 4: 337, 5: 464, 6: 397, 7: 267, 8: 492, 9: 251, 10: 793, 11: 641, 12: 687, 13: 1253, 14: 1416, 15: 4477, 16: 1814, 17: 1146, 18: 1439, 19: 1188, 20: 1722, 21: 1745, 22: 479, 23: 543}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "result_list = []\n",
    "\n",
    "\n",
    "\n",
    "for row in ask_posts:\n",
    "    created_at = row[6]\n",
    "    comments = row[4]\n",
    "    result_list.append([created_at, comments])\n",
    "\n",
    "# Test: check if loop produces desired data     \n",
    "print(result_list[:5])\n",
    "\n",
    "#We will now intialize dicitionaries, to track the number of posts in an hour\n",
    "#We will also count comments in a specific hour. \n",
    "# we will then use the two to analyze Comments per posts in a specific hour to gage popularity\n",
    "\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "#note : the used implementation of datetime does not require differentaion between padded and unpadded numbers\n",
    "\n",
    "for row in result_list:\n",
    "    hour = row[0]\n",
    "    comments = int(row[1])\n",
    "    hour_object = datetime.strptime(hour, '%m/%d/%Y %H:%M')\n",
    "    target_hour = hour_object.hour\n",
    "    if target_hour not in counts_by_hour:\n",
    "        counts_by_hour[target_hour] = 1\n",
    "        comments_by_hour[target_hour] = comments\n",
    "    else:\n",
    "        counts_by_hour[target_hour] += 1\n",
    "        comments_by_hour[target_hour] += comments\n",
    "\n",
    "print('\\n')        \n",
    "print(\"These are the post counts by hour:\\n\")\n",
    "print(counts_by_hour)\n",
    "print('\\n')   \n",
    "print(\"These are the comment counts by hour: \\n\")\n",
    "print(comments_by_hour)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now succesfully populated 2 dictionaries:\n",
    "\n",
    "*counts_by_hour* : contains the number of ask posts created during each hour of the day.\n",
    "\n",
    "*comments_by_hour* : contains the corresponding number of comments ask posts created at each hour received.\n",
    "\n",
    "Next, we'll use these two dictionaries to calculate the average number of comments for posts created during each hour of the day.\n",
    "\n",
    "To do so, we will create a list of lists, where the first entry is the hour and the second element is the average number of comments per post, in that hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The following list is the average comments per post , grouped by hour posted\n",
      "\n",
      "[0, 8.127272727272727]\n",
      "[1, 11.383333333333333]\n",
      "[2, 23.810344827586206]\n",
      "[3, 7.796296296296297]\n",
      "[4, 7.170212765957447]\n",
      "[5, 10.08695652173913]\n",
      "[6, 9.022727272727273]\n",
      "[7, 7.852941176470588]\n",
      "[8, 10.25]\n",
      "[9, 5.5777777777777775]\n",
      "[10, 13.440677966101696]\n",
      "[11, 11.051724137931034]\n",
      "[12, 9.41095890410959]\n",
      "[13, 14.741176470588234]\n",
      "[14, 13.233644859813085]\n",
      "[15, 38.5948275862069]\n",
      "[16, 16.796296296296298]\n",
      "[17, 11.46]\n",
      "[18, 13.20183486238532]\n",
      "[19, 10.8]\n",
      "[20, 21.525]\n",
      "[21, 16.009174311926607]\n",
      "[22, 6.746478873239437]\n",
      "[23, 7.985294117647059]\n"
     ]
    }
   ],
   "source": [
    "# first we create a list based on the dictionaries so that we can perform opertaions\n",
    "\n",
    "avg_by_hour = []\n",
    "\n",
    "\n",
    "for row in counts_by_hour:\n",
    "    avg_by_hour.append([row,comments_by_hour[row] / counts_by_hour[row]])\n",
    "\n",
    "print('\\n')\n",
    "print('The following list is the average comments per post , grouped by hour posted'+ '\\n')\n",
    "print(*avg_by_hour, sep = \"\\n\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now order the Produced list, to identify the best posting time. Listed printed above, can sometimes print unordered (although the python collision resolution wil usually print it in correct order, given that we have provided well-orderd inputs).\n",
    "\n",
    "We will then isolate the top 5 best posting hours, to summarize the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 38.5948275862069]\n",
      "[2, 23.810344827586206]\n",
      "[20, 21.525]\n",
      "[16, 16.796296296296298]\n",
      "[21, 16.009174311926607]\n",
      "[13, 14.741176470588234]\n",
      "[10, 13.440677966101696]\n",
      "[14, 13.233644859813085]\n",
      "[18, 13.20183486238532]\n",
      "[17, 11.46]\n",
      "[1, 11.383333333333333]\n",
      "[11, 11.051724137931034]\n",
      "[19, 10.8]\n",
      "[8, 10.25]\n",
      "[5, 10.08695652173913]\n",
      "[12, 9.41095890410959]\n",
      "[6, 9.022727272727273]\n",
      "[0, 8.127272727272727]\n",
      "[23, 7.985294117647059]\n",
      "[7, 7.852941176470588]\n",
      "[3, 7.796296296296297]\n",
      "[4, 7.170212765957447]\n",
      "[22, 6.746478873239437]\n",
      "[9, 5.5777777777777775]\n"
     ]
    }
   ],
   "source": [
    "# These 2 lines will sort the avg_by_hour list by the avg \n",
    "# post numer, reverse sort ( i.e descending) it and print it\n",
    "\n",
    "\n",
    "avg_by_hour.sort(key = lambda avg_by_hour: avg_by_hour[1], reverse = True) \n",
    "\n",
    "sorted_swap = avg_by_hour\n",
    "\n",
    "\n",
    "print(*sorted_swap, sep = \"\\n\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean up the presentation and display the top 5 candidate posting hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Hours to post for 'Ask' Posts based on Average Comments\n",
      "\n",
      "\n",
      "15:00 : 38.59 average comments per post\n",
      "2:00 : 23.81 average comments per post\n",
      "20:00 : 21.52 average comments per post\n",
      "16:00 : 16.80 average comments per post\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 5 Hours to post for 'Ask' Posts based on Average Comments\")\n",
    "print('\\n')\n",
    "\n",
    "for row in sorted_swap[:4]:\n",
    "    output = \"{}:00 : {:.2f} average comments per post\".format(str(row[0]), row[1])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Also note that this is based on US Eastern time. Based on the data documentation. This also the project author's (Matt's) time zone.\n",
    "\n",
    "We can see that there are no morning hours for the post times, and the most populaar post hour is 15:00. This might be due to the fact, that the portal itself will have low readership during work hours and earlier.\n",
    "\n",
    "Posts posted after 15:00 will have the benefit of gaining traffic and being visibility once everyone finishes work in the east coast. Once traction has been gained, traffic could inflow from the west coast in the evening as well - so the effects would accumulate. \n",
    "\n",
    "An interesting finding is that posts sent at 2 in the morning are quite popular - this would require deeper investigation. It could be that, these posts would be ready for the early morning readers, and upvoted by any readers that have picked it up at night. \n",
    "\n",
    "That being said, the analysis of these posts is based on averages for times, and comments. There other factors to gage post popularity such as social media shares, and the quality of the article itself. The effect of this time is indicative of a correlation - not neccessarily a strong causal link.\n",
    "\n",
    "However, this top 5 could give an inidication of when to post an 'Ask' article, to get the most traction on average. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
